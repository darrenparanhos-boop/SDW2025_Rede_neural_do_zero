# ğŸ§  Rede Neural do Zero com PyTorch

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/10Jb6rMh27h-tgDsE5Yx6iBM3oOMjsvlO?usp=sharing)

Este projeto apresenta a implementaÃ§Ã£o de uma **rede neural totalmente conectada (MLP)** desenvolvida do zero utilizando **PyTorch** e o dataset **MNIST**.  
O estudo foi realizado no **Google Colab** e documentado no notebook [`Rede_neural_do_zero.ipynb`](https://colab.research.google.com/drive/10Jb6rMh27h-tgDsE5Yx6iBM3oOMjsvlO?usp=sharing).

---

## ğŸ“Œ Objetivos
- Implementar uma rede neural simples para classificaÃ§Ã£o de dÃ­gitos manuscritos.  
- Explorar conceitos fundamentais de **camadas lineares**, **funÃ§Ãµes de ativaÃ§Ã£o** e **otimizaÃ§Ã£o**.  
- Demonstrar o fluxo completo: **prÃ©-processamento â†’ modelagem â†’ treinamento â†’ validaÃ§Ã£o**.  

---

## ğŸ“š Tecnologias Utilizadas
- **Python 3.14**
- **PyTorch**
- **Torchvision**
- **Matplotlib**
- **NumPy**

---

## ğŸ“Š Dataset MNIST
O dataset **MNIST** contÃ©m imagens de dÃ­gitos manuscritos (0â€“9) em escala de cinza:

- **Treino:** 60.000 imagens  
- **Teste/ValidaÃ§Ã£o:** 10.000 imagens  
- **DimensÃ£o:** 28x28 pixels  

Exemplo de visualizaÃ§Ã£o de uma imagem:

```python
plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')
plt.title(f"Label: {labels[0].item()}")
plt.show()
```

---

## ğŸ—ï¸ Arquitetura da Rede Neural
A rede neural implementada possui trÃªs camadas **fully connected**:

```
Entrada (784 neurÃ´nios - pixels 28x28)
            â”‚
            â–¼
     [Camada Oculta 1]
        128 neurÃ´nios
            â”‚
            â–¼
     [Camada Oculta 2]
         64 neurÃ´nios
            â”‚
            â–¼
     [Camada de SaÃ­da]
        10 neurÃ´nios (dÃ­gitos 0â€“9)
```

- **FunÃ§Ãµes de ativaÃ§Ã£o:**
  - ReLU nas camadas internas  
  - LogSoftmax na camada de saÃ­da  

---

## âš™ï¸ Treinamento
- **Otimizador:** `SGD` (Stochastic Gradient Descent) com `lr=0.01` e `momentum=0.5`  
- **FunÃ§Ã£o de perda:** `NLLLoss`  
- **Ã‰pocas:** 10  

Durante o treinamento, a perda acumulada por Ã©poca Ã© exibida:

```text
Epoch 1 - Perda resultante: ...
Epoch 2 - Perda resultante: ...
...
Tempo de treino (em minutos) = ...
```

---

## âœ… ValidaÃ§Ã£o
A validaÃ§Ã£o Ã© realizada com o conjunto de teste.  
O modelo calcula a probabilidade de cada classe e compara com o rÃ³tulo correto.  
Ao final, Ã© exibida a **precisÃ£o total**:

```text
Total de imagens testadas = 10000
PrecisÃ£o do modelo = XX%
```

---

## ğŸ’» ExecuÃ§Ã£o em GPU/CPU
O cÃ³digo detecta automaticamente se hÃ¡ GPU disponÃ­vel (CUDA). Caso contrÃ¡rio, roda em CPU:

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
modelo.to(device)
```

---

## ğŸš€ Como Executar
1. Clique no badge **Open in Colab** no topo ou acesse diretamente [este link](https://colab.research.google.com/drive/10Jb6rMh27h-tgDsE5Yx6iBM3oOMjsvlO?usp=sharing).  
2. Certifique-se de que o dataset MNIST serÃ¡ baixado automaticamente.  
3. Execute as cÃ©lulas em ordem para treinar e validar o modelo.  

---

## ğŸ“ˆ Resultados Esperados
- VisualizaÃ§Ã£o de exemplos do dataset MNIST.  
- Treinamento da rede neural com reduÃ§Ã£o progressiva da perda.  
- ValidaÃ§Ã£o com precisÃ£o aproximada entre **92% e 95%** (dependendo da configuraÃ§Ã£o e nÃºmero de Ã©pocas).  

---

## ğŸ”® PrÃ³ximos Passos
- Implementar **camadas convolucionais (CNNs)** para melhorar a precisÃ£o.  
- Testar diferentes otimizadores como **Adam**.  
- Aumentar o nÃºmero de Ã©pocas e ajustar hiperparÃ¢metros.  
- Comparar desempenho entre CPU e GPU.  

---

## ğŸ“‚ Estrutura do Projeto
```
â”œâ”€â”€ Rede_neural_do_zero.ipynb   # Notebook principal
â”œâ”€â”€ MNIST_data/                 # Dataset baixado automaticamente
â””â”€â”€ README.md                   # DocumentaÃ§Ã£o do projeto
```

---

## ğŸ‘¨â€ğŸ’» Autor
Projeto desenvolvido como estudo prÃ¡tico de **Redes Neurais com PyTorch**.  

---

## ğŸ“ ConclusÃ£o
Este estudo demonstrou, de forma prÃ¡tica e estruturada, como construir uma rede neural totalmente conectada (MLP) utilizando PyTorch para resolver o desafio clÃ¡ssico de classificaÃ§Ã£o de dÃ­gitos manuscritos com o dataset MNIST.
A implementaÃ§Ã£o percorreu todas as etapas fundamentais do ciclo de aprendizado de mÃ¡quina:

â€¢ 	PrÃ©-processamento dos dados, garantindo que as imagens fossem convertidas em tensores adequados para o modelo.

â€¢ 	DefiniÃ§Ã£o da arquitetura, com camadas lineares e funÃ§Ãµes de ativaÃ§Ã£o que refletem os princÃ­pios bÃ¡sicos das redes neurais.

â€¢ 	Treinamento supervisionado, utilizando gradiente descendente estocÃ¡stico para otimizaÃ§Ã£o dos parÃ¢metros.

â€¢ 	ValidaÃ§Ã£o e avaliaÃ§Ã£o, assegurando que o modelo fosse testado em dados nÃ£o vistos e medindo sua capacidade de generalizaÃ§Ã£o.

Os resultados obtidos confirmam que, mesmo com uma arquitetura simples, Ã© possÃ­vel alcanÃ§ar alta precisÃ£o (92â€“95%), evidenciando o potencial das redes neurais para tarefas de classificaÃ§Ã£o.
Mais do que apenas treinar um modelo, este estudo reforÃ§a conceitos essenciais de aprendizado profundo, servindo como base sÃ³lida para avanÃ§os futuros, tais como:

â€¢ 	ImplementaÃ§Ã£o de redes convolucionais (CNNs) para maior desempenho em visÃ£o computacional.

â€¢ 	ExploraÃ§Ã£o de diferentes otimizadores e hiperparÃ¢metros para melhorar a eficiÃªncia do treinamento.

â€¢ 	AplicaÃ§Ãµes em datasets mais complexos e variados, ampliando o escopo e a robustez do modelo.

Em sÃ­ntese, este projeto cumpre seu papel como um primeiro passo consistente na jornada de aprendizado em redes neurais, oferecendo uma visÃ£o clara e prÃ¡tica de como modelos de machine learning podem ser construÃ­dos, treinados e aplicados em problemas reais.
Ele nÃ£o apenas consolida os fundamentos teÃ³ricos, mas tambÃ©m abre caminho para experimentaÃ§Ãµes mais avanÃ§adas e aplicaÃ§Ãµes em cenÃ¡rios de maior complexidade.


